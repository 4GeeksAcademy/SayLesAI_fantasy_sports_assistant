{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling\n",
    "\n",
    "Since the data tables on the [Footballguys](https://www.footballguys.com/) website are fully rendered in HTML, we might be able to scrape the data without too much trouble. This gives us good control over exactly what data we download and an easy mechanism by which to update it throughout the season. Let's give it a try using [urllib](https://docs.python.org/3/howto/urllib2.html) and [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "from itertools import product\n",
    "from random import randrange\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Decide if we want to re-download the data or not\n",
    "download_data=False\n",
    "\n",
    "# Set the data file paths\n",
    "raw_data_path='../data/raw_qb_data.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and parse HTML data\n",
    "\n",
    "The available data spans 1996 to 2024 and each year has 18 weeks of data. We also will want to download the data for multiple positions. But, let's start with just one. We also need to pick a scoring scheme, let's go with PPR. We can easily change this later. We will use a loop to construct and download the URL for each year and week and parse and collect the data as we get it.\n",
    "\n",
    "**Note**: Downloading all of the data for one position takes just over 45 minutes.\n",
    "\n",
    "### 1.1. Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url: str) -> bytes:\n",
    "    '''Takes string url, downloads URL and returns HTML bytes object'''\n",
    "\n",
    "    headers={\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Host\": \"httpbin.io\",\n",
    "        \"Sec-Ch-Ua\": '\"Google Chrome\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\"',\n",
    "        \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "        \"Sec-Ch-Ua-Platform\": '\"Linux\"',\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"cross-site\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Create the request\n",
    "    request_params = urllib.request.Request(\n",
    "        url=url,\n",
    "        headers=headers\n",
    "    )   \n",
    "\n",
    "    # Get the html\n",
    "    with urllib.request.urlopen(request_params) as response:\n",
    "        html=response.read()\n",
    "\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. HTML parsing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_table(html: bytes, position: str, year: int, week: int, profile: str) -> pd.DataFrame:\n",
    "    '''Takes a html bytes object from URL, parses data table, adds\n",
    "    year, week, position and scoring profile and returns as pandas dataframe'''\n",
    "\n",
    "    # Extract the table rows\n",
    "    soup=BeautifulSoup(html, 'html.parser')\n",
    "    table=soup.find('table',{'class':'datasmall table'})\n",
    "    table_rows=table.find_all('tr')\n",
    "\n",
    "    # Get the column names from the first row\n",
    "    columns=table_rows[0].find_all('th')\n",
    "    column_names=[column.getText() for column in columns]\n",
    "    column_names.extend(['Position', 'Year', 'Week', 'Scoring profile'])\n",
    "\n",
    "    # Get the values for each row\n",
    "    data=[]\n",
    "\n",
    "    for row in table_rows[1:]:\n",
    "        columns=row.find_all('td')\n",
    "        values=[column.getText() for column in columns]\n",
    "        values.extend([position, year, week, profile])\n",
    "        data.append(values)\n",
    "\n",
    "    # Convert to pandas dataframe and return\n",
    "    return pd.DataFrame(columns=column_names, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Main download loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL parameter arguments\n",
    "position='qb'\n",
    "profile='p'\n",
    "years=list(range(1996,2025))\n",
    "weeks=list(range(1,19))\n",
    "\n",
    "# Download the data if asked\n",
    "if download_data is True:\n",
    "\n",
    "    # Empty list to accumulate results\n",
    "    results=[]\n",
    "\n",
    "    for year, week in product(years, weeks):\n",
    "\n",
    "        print(f'Downloading data for {year}', end='\\r')\n",
    "\n",
    "        # Construct the URL for this year and week\n",
    "        url=f'https://www.footballguys.com/playerhistoricalstats?pos={position}&yr={year}&startwk={week}&stopwk={week}&profile={profile}'\n",
    "\n",
    "        # Get the HTML\n",
    "        html=download_url(url)\n",
    "\n",
    "        # Parse the HTML\n",
    "        result=parse_html_table(html, position, year, week, profile)\n",
    "\n",
    "        # Collect the result\n",
    "        results.append(result)\n",
    "\n",
    "        # Wait before downloading the next page\n",
    "        time.sleep(randrange(1, 10))\n",
    "\n",
    "    # Combine the week by week dataframes\n",
    "    data_df=pd.concat(results)\n",
    "\n",
    "    # Clean up the index\n",
    "    data_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Save as parquet\n",
    "    data_df.to_parquet(raw_data_path)\n",
    "\n",
    "# Or load it from disk if we already have it\n",
    "if download_data is False:\n",
    "    data_df=pd.read_parquet(raw_data_path)\n",
    "    print('Loaded data from disk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fix the player name/team column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Team']=data_df['Name'].apply(lambda x: x.split()[-1])\n",
    "data_df['Name']=data_df['Name'].apply(lambda x: ' '.join(x.split()[:-1]))\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data reshaping\n",
    "\n",
    "Now let's get to work transforming this data. We will treat each player as an independent sample within each year, this will give us a set of time series for quarterback performance across a season. The data structure here might be ragged, meaning that the rows are not all the same length. But, I think that is better than trying to put in zeros or placeholder values for the weeks that a player didn't play. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lists of independent variable\n",
    "years=data_df['Year'].unique()\n",
    "players=data_df['Name'].unique()\n",
    "\n",
    "print(f'Have data for {len(players)} players\\n')\n",
    "\n",
    "# Empty list to hold data\n",
    "data=[]\n",
    "\n",
    "# Loop on the year and the player\n",
    "for year in years:\n",
    "\n",
    "    # Empty list to hold data for this year\n",
    "    year_data=[]\n",
    "\n",
    "    for player in players:\n",
    "\n",
    "        # Empty list to hold data for this player\n",
    "        player_data=[]\n",
    "\n",
    "        # Get data for this year and player\n",
    "        year_player_df=data_df[(data_df['Year'] == year) & (data_df['Name'] == player)]\n",
    "        print(f'Have {len(year_player_df)} weeks of data for {player} in {year}')\n",
    "\n",
    "        # Drop features we don't want\n",
    "        year_player_df.drop(['Rank', 'Name', 'Age', 'Position', 'Year', 'Week', 'Scoring profile', 'Team'], axis=1, inplace=True)\n",
    "\n",
    "        # Loop on the weeks (rows) and collect this players stats\n",
    "        for i, row in year_player_df.iterrows():\n",
    "            \n",
    "            # Add the row to the player data\n",
    "            player_data.append(row.tolist())\n",
    "\n",
    "        print(player_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
