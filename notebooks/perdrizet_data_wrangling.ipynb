{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling\n",
    "\n",
    "Since the data tables on the [Footballguys](https://www.footballguys.com/) website are fully rendered in HTML, we might be able to scrape the data without too much trouble. This gives us good control over exactly what data we download and an easy mechanism by which to update it throughout the season. Let's give it a try using [urllib](https://docs.python.org/3/howto/urllib2.html) and [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "from itertools import product\n",
    "from random import randrange\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and parse HTML data\n",
    "\n",
    "The available data spans 1996 to 2024 and each year has 18 weeks of data. We also will want to download the data for multiple positions. But, let's start with just one. We also need to pick a scoring scheme, let's go with PPR. We can easily change this later. We will use a loop to construct and download the URL for each year and week and parse and collect the data as we get it.\n",
    "\n",
    "**Note**: Downloading all of the data for one position takes just over 45 minutes.\n",
    "\n",
    "### 1.1. Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url: str) -> bytes:\n",
    "    '''Takes string url, downloads URL and returns HTML bytes object'''\n",
    "\n",
    "    headers={\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Host\": \"httpbin.io\",\n",
    "        \"Sec-Ch-Ua\": '\"Google Chrome\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\"',\n",
    "        \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "        \"Sec-Ch-Ua-Platform\": '\"Linux\"',\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"cross-site\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Create the request\n",
    "    request_params = urllib.request.Request(\n",
    "        url=url,\n",
    "        headers=headers\n",
    "    )   \n",
    "\n",
    "    # Get the html\n",
    "    with urllib.request.urlopen(request_params) as response:\n",
    "        html=response.read()\n",
    "\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. HTML parsing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_table(html: bytes, year: int, week: int, position: str, profile: str) -> pd.DataFrame:\n",
    "    '''Takes a html bytes object from URL, parses data table, adds\n",
    "    year, week, position and scoring profile and returns as pandas dataframe'''\n",
    "\n",
    "    # Extract the table rows\n",
    "    soup=BeautifulSoup(html, 'html.parser')\n",
    "    table=soup.find('table',{'class':'datasmall table'})\n",
    "    table_rows=table.find_all('tr')\n",
    "\n",
    "    # Get the column names from the first row\n",
    "    columns=table_rows[0].find_all('th')\n",
    "    column_names=[column.getText() for column in columns]\n",
    "    column_names.extend(['Position', 'Year', 'Week', 'Scoring profile'])\n",
    "\n",
    "    # Get the values for each row\n",
    "    data=[]\n",
    "\n",
    "    for row in table_rows[1:]:\n",
    "        columns=row.find_all('td')\n",
    "        values=[column.getText() for column in columns]\n",
    "        values.extend([position, year, week, profile])\n",
    "        data.append(values)\n",
    "\n",
    "    # Convert to pandas dataframe and return\n",
    "    return pd.DataFrame(columns=column_names, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Main download loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading QB, 2020, week 1\n",
      "Downloading QB, 2020, week 2\n",
      "Downloading QB, 2020, week 3\n",
      "Downloading QB, 2020, week 4\n",
      "Downloading QB, 2020, week 5\n",
      "Downloading QB, 2020, week 6\n",
      "Downloading QB, 2020, week 7\n",
      "Downloading QB, 2020, week 8\n",
      "Downloading QB, 2020, week 9\n",
      "Downloading QB, 2020, week 10\n",
      "Downloading QB, 2020, week 11\n"
     ]
    }
   ],
   "source": [
    "# Main script to download data\n",
    "download_data = True  # Or False, depending on what you want to do\n",
    "\n",
    "if download_data is True:\n",
    "    positions = ['qb', 'rb', 'wr', 'te']\n",
    "    profile = 'p'\n",
    "    years = list(range(2020, 2024))\n",
    "    weeks = list(range(1, 19))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for position, year, week in product(positions, years, weeks):\n",
    "        print(f'Downloading {position.upper()}, {year}, week {week}')\n",
    "        url = f'https://www.footballguys.com/playerhistoricalstats?pos={position}&yr={year}&startwk={week}&stopwk={week}&profile={profile}'\n",
    "        \n",
    "        # Get the HTML\n",
    "        html = download_url(url)\n",
    "        \n",
    "        # Parse the HTML\n",
    "        result = parse_html_table(html, position, year, week, profile)\n",
    "        \n",
    "        # Collect the result\n",
    "        results.append(result)\n",
    "\n",
    "        # Wait before downloading the next page\n",
    "        time.sleep(randrange(1, 5))\n",
    "\n",
    "    # Combine the week-by-week dataframes\n",
    "    data_df = pd.concat(results)\n",
    "\n",
    "elif download_data is False:\n",
    "    data_df = pd.read_parquet('../data/raw_qb_data.parquet')\n",
    "    print('Loaded data from file')\n",
    "\n",
    "# View the resulting DataFrame\n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fix the player name/team column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mdata_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      2\u001b[0m data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdata_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x\u001b[38;5;241m.\u001b[39msplit()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m      3\u001b[0m data_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "data_df['Team']=data_df['Name'].apply(lambda x: x.split()[-1])\n",
    "data_df['Name']=data_df['Name'].apply(lambda x: ' '.join(x.split()[:-1]))\n",
    "data_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17938 entries, 0 to 17937\n",
      "Data columns (total 22 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Rank             17938 non-null  object\n",
      " 1   Name             17938 non-null  object\n",
      " 2   Age              17938 non-null  object\n",
      " 3   Exp              17938 non-null  object\n",
      " 4   G                17938 non-null  object\n",
      " 5   Cmp              17938 non-null  object\n",
      " 6   Att              17938 non-null  object\n",
      " 7   Cm%              17938 non-null  object\n",
      " 8   PYd              17938 non-null  object\n",
      " 9   Y/Att            17938 non-null  object\n",
      " 10  PTD              17938 non-null  object\n",
      " 11  Int              17938 non-null  object\n",
      " 12  Rsh              17938 non-null  object\n",
      " 13  RshYd            17938 non-null  object\n",
      " 14  RshTD            17938 non-null  object\n",
      " 15  FP/G             17938 non-null  object\n",
      " 16  FantPt           17938 non-null  object\n",
      " 17  Position         17938 non-null  object\n",
      " 18  Year             17938 non-null  object\n",
      " 19  Week             17938 non-null  object\n",
      " 20  Scoring profile  17938 non-null  object\n",
      " 21  Team             17938 non-null  object\n",
      "dtypes: object(22)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# First, clean up the index and take a look at what we have:\n",
    "data_df.reset_index(inplace=True, drop=True)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as parquet\n",
    "data_df.to_parquet('../data/raw_qb_data.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
